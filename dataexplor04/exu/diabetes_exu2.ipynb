{
 "metadata": {
  "name": "",
  "signature": "sha256:9329b903c74d01394b7f6d0d442825bc143b83a2a92fca7b2c950536eb9f112e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_diabetes\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "# import scipy as sp\n",
      "# import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import Lasso\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import f_regression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.cross_validation import KFold\n",
      "# %matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The diabetes dataset consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442\n",
      "patients, and an indication of disease progression after one year:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = load_diabetes()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df=pd.DataFrame(data.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['y']=data.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "      <th>2</th>\n",
        "      <th>3</th>\n",
        "      <th>4</th>\n",
        "      <th>5</th>\n",
        "      <th>6</th>\n",
        "      <th>7</th>\n",
        "      <th>8</th>\n",
        "      <th>9</th>\n",
        "      <th>y</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 4.420000e+02</td>\n",
        "      <td> 4.420000e+02</td>\n",
        "      <td> 4.420000e+02</td>\n",
        "      <td> 4.420000e+02</td>\n",
        "      <td> 4.420000e+02</td>\n",
        "      <td> 4.420000e+02</td>\n",
        "      <td> 4.420000e+02</td>\n",
        "      <td> 4.420000e+02</td>\n",
        "      <td> 4.420000e+02</td>\n",
        "      <td> 4.420000e+02</td>\n",
        "      <td> 442.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>-3.639623e-16</td>\n",
        "      <td> 1.309912e-16</td>\n",
        "      <td>-8.013951e-16</td>\n",
        "      <td> 1.289818e-16</td>\n",
        "      <td>-9.042540e-17</td>\n",
        "      <td> 1.301121e-16</td>\n",
        "      <td>-4.563971e-16</td>\n",
        "      <td> 3.863174e-16</td>\n",
        "      <td>-3.848103e-16</td>\n",
        "      <td>-3.398488e-16</td>\n",
        "      <td> 152.133484</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td> 4.761905e-02</td>\n",
        "      <td> 4.761905e-02</td>\n",
        "      <td> 4.761905e-02</td>\n",
        "      <td> 4.761905e-02</td>\n",
        "      <td> 4.761905e-02</td>\n",
        "      <td> 4.761905e-02</td>\n",
        "      <td> 4.761905e-02</td>\n",
        "      <td> 4.761905e-02</td>\n",
        "      <td> 4.761905e-02</td>\n",
        "      <td> 4.761905e-02</td>\n",
        "      <td>  77.093005</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>-1.072256e-01</td>\n",
        "      <td>-4.464164e-02</td>\n",
        "      <td>-9.027530e-02</td>\n",
        "      <td>-1.123996e-01</td>\n",
        "      <td>-1.267807e-01</td>\n",
        "      <td>-1.156131e-01</td>\n",
        "      <td>-1.023071e-01</td>\n",
        "      <td>-7.639450e-02</td>\n",
        "      <td>-1.260974e-01</td>\n",
        "      <td>-1.377672e-01</td>\n",
        "      <td>  25.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>-3.729927e-02</td>\n",
        "      <td>-4.464164e-02</td>\n",
        "      <td>-3.422907e-02</td>\n",
        "      <td>-3.665645e-02</td>\n",
        "      <td>-3.424784e-02</td>\n",
        "      <td>-3.035840e-02</td>\n",
        "      <td>-3.511716e-02</td>\n",
        "      <td>-3.949338e-02</td>\n",
        "      <td>-3.324879e-02</td>\n",
        "      <td>-3.317903e-02</td>\n",
        "      <td>  87.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td> 5.383060e-03</td>\n",
        "      <td>-4.464164e-02</td>\n",
        "      <td>-7.283766e-03</td>\n",
        "      <td>-5.670611e-03</td>\n",
        "      <td>-4.320866e-03</td>\n",
        "      <td>-3.819065e-03</td>\n",
        "      <td>-6.584468e-03</td>\n",
        "      <td>-2.592262e-03</td>\n",
        "      <td>-1.947634e-03</td>\n",
        "      <td>-1.077698e-03</td>\n",
        "      <td> 140.500000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td> 3.807591e-02</td>\n",
        "      <td> 5.068012e-02</td>\n",
        "      <td> 3.124802e-02</td>\n",
        "      <td> 3.564384e-02</td>\n",
        "      <td> 2.835801e-02</td>\n",
        "      <td> 2.984439e-02</td>\n",
        "      <td> 2.931150e-02</td>\n",
        "      <td> 3.430886e-02</td>\n",
        "      <td> 3.243323e-02</td>\n",
        "      <td> 2.791705e-02</td>\n",
        "      <td> 211.500000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td> 1.107267e-01</td>\n",
        "      <td> 5.068012e-02</td>\n",
        "      <td> 1.705552e-01</td>\n",
        "      <td> 1.320442e-01</td>\n",
        "      <td> 1.539137e-01</td>\n",
        "      <td> 1.987880e-01</td>\n",
        "      <td> 1.811791e-01</td>\n",
        "      <td> 1.852344e-01</td>\n",
        "      <td> 1.335990e-01</td>\n",
        "      <td> 1.356118e-01</td>\n",
        "      <td> 346.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "                  0             1             2             3             4  \\\n",
        "count  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02   \n",
        "mean  -3.639623e-16  1.309912e-16 -8.013951e-16  1.289818e-16 -9.042540e-17   \n",
        "std    4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02   \n",
        "min   -1.072256e-01 -4.464164e-02 -9.027530e-02 -1.123996e-01 -1.267807e-01   \n",
        "25%   -3.729927e-02 -4.464164e-02 -3.422907e-02 -3.665645e-02 -3.424784e-02   \n",
        "50%    5.383060e-03 -4.464164e-02 -7.283766e-03 -5.670611e-03 -4.320866e-03   \n",
        "75%    3.807591e-02  5.068012e-02  3.124802e-02  3.564384e-02  2.835801e-02   \n",
        "max    1.107267e-01  5.068012e-02  1.705552e-01  1.320442e-01  1.539137e-01   \n",
        "\n",
        "                  5             6             7             8             9  \\\n",
        "count  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02   \n",
        "mean   1.301121e-16 -4.563971e-16  3.863174e-16 -3.848103e-16 -3.398488e-16   \n",
        "std    4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02   \n",
        "min   -1.156131e-01 -1.023071e-01 -7.639450e-02 -1.260974e-01 -1.377672e-01   \n",
        "25%   -3.035840e-02 -3.511716e-02 -3.949338e-02 -3.324879e-02 -3.317903e-02   \n",
        "50%   -3.819065e-03 -6.584468e-03 -2.592262e-03 -1.947634e-03 -1.077698e-03   \n",
        "75%    2.984439e-02  2.931150e-02  3.430886e-02  3.243323e-02  2.791705e-02   \n",
        "max    1.987880e-01  1.811791e-01  1.852344e-01  1.335990e-01  1.356118e-01   \n",
        "\n",
        "                y  \n",
        "count  442.000000  \n",
        "mean   152.133484  \n",
        "std     77.093005  \n",
        "min     25.000000  \n",
        "25%     87.000000  \n",
        "50%    140.500000  \n",
        "75%    211.500000  \n",
        "max    346.000000  "
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "      <th>2</th>\n",
        "      <th>3</th>\n",
        "      <th>4</th>\n",
        "      <th>5</th>\n",
        "      <th>6</th>\n",
        "      <th>7</th>\n",
        "      <th>8</th>\n",
        "      <th>9</th>\n",
        "      <th>y</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0.038076</td>\n",
        "      <td> 0.050680</td>\n",
        "      <td> 0.061696</td>\n",
        "      <td> 0.021872</td>\n",
        "      <td>-0.044223</td>\n",
        "      <td>-0.034821</td>\n",
        "      <td>-0.043401</td>\n",
        "      <td>-0.002592</td>\n",
        "      <td> 0.019908</td>\n",
        "      <td>-0.017646</td>\n",
        "      <td> 151</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>-0.001882</td>\n",
        "      <td>-0.044642</td>\n",
        "      <td>-0.051474</td>\n",
        "      <td>-0.026328</td>\n",
        "      <td>-0.008449</td>\n",
        "      <td>-0.019163</td>\n",
        "      <td> 0.074412</td>\n",
        "      <td>-0.039493</td>\n",
        "      <td>-0.068330</td>\n",
        "      <td>-0.092204</td>\n",
        "      <td>  75</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 0.085299</td>\n",
        "      <td> 0.050680</td>\n",
        "      <td> 0.044451</td>\n",
        "      <td>-0.005671</td>\n",
        "      <td>-0.045599</td>\n",
        "      <td>-0.034194</td>\n",
        "      <td>-0.032356</td>\n",
        "      <td>-0.002592</td>\n",
        "      <td> 0.002864</td>\n",
        "      <td>-0.025930</td>\n",
        "      <td> 141</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>-0.089063</td>\n",
        "      <td>-0.044642</td>\n",
        "      <td>-0.011595</td>\n",
        "      <td>-0.036656</td>\n",
        "      <td> 0.012191</td>\n",
        "      <td> 0.024991</td>\n",
        "      <td>-0.036038</td>\n",
        "      <td> 0.034309</td>\n",
        "      <td> 0.022692</td>\n",
        "      <td>-0.009362</td>\n",
        "      <td> 206</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 0.005383</td>\n",
        "      <td>-0.044642</td>\n",
        "      <td>-0.036385</td>\n",
        "      <td> 0.021872</td>\n",
        "      <td> 0.003935</td>\n",
        "      <td> 0.015596</td>\n",
        "      <td> 0.008142</td>\n",
        "      <td>-0.002592</td>\n",
        "      <td>-0.031991</td>\n",
        "      <td>-0.046641</td>\n",
        "      <td> 135</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "          0         1         2         3         4         5         6  \\\n",
        "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
        "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
        "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
        "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
        "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
        "\n",
        "          7         8         9    y  \n",
        "0 -0.002592  0.019908 -0.017646  151  \n",
        "1 -0.039493 -0.068330 -0.092204   75  \n",
        "2 -0.002592  0.002864 -0.025930  141  \n",
        "3  0.034309  0.022692 -0.009362  206  \n",
        "4 -0.002592 -0.031991 -0.046641  135  "
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Examine the Correlation Between Predictors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predictors = [col for col in df.columns if col != 'y']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df[predictors].corr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "      <th>2</th>\n",
        "      <th>3</th>\n",
        "      <th>4</th>\n",
        "      <th>5</th>\n",
        "      <th>6</th>\n",
        "      <th>7</th>\n",
        "      <th>8</th>\n",
        "      <th>9</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 0.173737</td>\n",
        "      <td> 0.185085</td>\n",
        "      <td> 0.335427</td>\n",
        "      <td> 0.260061</td>\n",
        "      <td> 0.219243</td>\n",
        "      <td>-0.075181</td>\n",
        "      <td> 0.203841</td>\n",
        "      <td> 0.270777</td>\n",
        "      <td> 0.301731</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 0.173737</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 0.088161</td>\n",
        "      <td> 0.241013</td>\n",
        "      <td> 0.035277</td>\n",
        "      <td> 0.142637</td>\n",
        "      <td>-0.379090</td>\n",
        "      <td> 0.332115</td>\n",
        "      <td> 0.149918</td>\n",
        "      <td> 0.208133</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 0.185085</td>\n",
        "      <td> 0.088161</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 0.395415</td>\n",
        "      <td> 0.249777</td>\n",
        "      <td> 0.261170</td>\n",
        "      <td>-0.366811</td>\n",
        "      <td> 0.413807</td>\n",
        "      <td> 0.446159</td>\n",
        "      <td> 0.388680</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 0.335427</td>\n",
        "      <td> 0.241013</td>\n",
        "      <td> 0.395415</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 0.242470</td>\n",
        "      <td> 0.185558</td>\n",
        "      <td>-0.178761</td>\n",
        "      <td> 0.257653</td>\n",
        "      <td> 0.393478</td>\n",
        "      <td> 0.390429</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 0.260061</td>\n",
        "      <td> 0.035277</td>\n",
        "      <td> 0.249777</td>\n",
        "      <td> 0.242470</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 0.896663</td>\n",
        "      <td> 0.051519</td>\n",
        "      <td> 0.542207</td>\n",
        "      <td> 0.515501</td>\n",
        "      <td> 0.325717</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td> 0.219243</td>\n",
        "      <td> 0.142637</td>\n",
        "      <td> 0.261170</td>\n",
        "      <td> 0.185558</td>\n",
        "      <td> 0.896663</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td>-0.196455</td>\n",
        "      <td> 0.659817</td>\n",
        "      <td> 0.318353</td>\n",
        "      <td> 0.290600</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>-0.075181</td>\n",
        "      <td>-0.379090</td>\n",
        "      <td>-0.366811</td>\n",
        "      <td>-0.178761</td>\n",
        "      <td> 0.051519</td>\n",
        "      <td>-0.196455</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td>-0.738493</td>\n",
        "      <td>-0.398577</td>\n",
        "      <td>-0.273697</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td> 0.203841</td>\n",
        "      <td> 0.332115</td>\n",
        "      <td> 0.413807</td>\n",
        "      <td> 0.257653</td>\n",
        "      <td> 0.542207</td>\n",
        "      <td> 0.659817</td>\n",
        "      <td>-0.738493</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 0.617857</td>\n",
        "      <td> 0.417212</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td> 0.270777</td>\n",
        "      <td> 0.149918</td>\n",
        "      <td> 0.446159</td>\n",
        "      <td> 0.393478</td>\n",
        "      <td> 0.515501</td>\n",
        "      <td> 0.318353</td>\n",
        "      <td>-0.398577</td>\n",
        "      <td> 0.617857</td>\n",
        "      <td> 1.000000</td>\n",
        "      <td> 0.464670</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td> 0.301731</td>\n",
        "      <td> 0.208133</td>\n",
        "      <td> 0.388680</td>\n",
        "      <td> 0.390429</td>\n",
        "      <td> 0.325717</td>\n",
        "      <td> 0.290600</td>\n",
        "      <td>-0.273697</td>\n",
        "      <td> 0.417212</td>\n",
        "      <td> 0.464670</td>\n",
        "      <td> 1.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "          0         1         2         3         4         5         6  \\\n",
        "0  1.000000  0.173737  0.185085  0.335427  0.260061  0.219243 -0.075181   \n",
        "1  0.173737  1.000000  0.088161  0.241013  0.035277  0.142637 -0.379090   \n",
        "2  0.185085  0.088161  1.000000  0.395415  0.249777  0.261170 -0.366811   \n",
        "3  0.335427  0.241013  0.395415  1.000000  0.242470  0.185558 -0.178761   \n",
        "4  0.260061  0.035277  0.249777  0.242470  1.000000  0.896663  0.051519   \n",
        "5  0.219243  0.142637  0.261170  0.185558  0.896663  1.000000 -0.196455   \n",
        "6 -0.075181 -0.379090 -0.366811 -0.178761  0.051519 -0.196455  1.000000   \n",
        "7  0.203841  0.332115  0.413807  0.257653  0.542207  0.659817 -0.738493   \n",
        "8  0.270777  0.149918  0.446159  0.393478  0.515501  0.318353 -0.398577   \n",
        "9  0.301731  0.208133  0.388680  0.390429  0.325717  0.290600 -0.273697   \n",
        "\n",
        "          7         8         9  \n",
        "0  0.203841  0.270777  0.301731  \n",
        "1  0.332115  0.149918  0.208133  \n",
        "2  0.413807  0.446159  0.388680  \n",
        "3  0.257653  0.393478  0.390429  \n",
        "4  0.542207  0.515501  0.325717  \n",
        "5  0.659817  0.318353  0.290600  \n",
        "6 -0.738493 -0.398577 -0.273697  \n",
        "7  1.000000  0.617857  0.417212  \n",
        "8  0.617857  1.000000  0.464670  \n",
        "9  0.417212  0.464670  1.000000  "
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks like there are pretty high correlations between 4 and 5, 5 and 7, 6 and 7."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = df[predictors].values\n",
      "y = df.y.values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Construct a baseline regressor with all features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reg = LinearRegression()\n",
      "reg.fit(X, y)\n",
      "ypred = reg.predict(X)\n",
      "rmse = np.sqrt(mean_squared_error(ypred, y))\n",
      "print(\"RMSE: %0.2f\" % (rmse.mean()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RMSE: 53.48\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Use SelectKBest to find the best subset regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mses = []\n",
      "nfeatures = range(1, len(predictors))\n",
      "\n",
      "for nfeature in nfeatures:\n",
      "    # compute MSE for different values of k (top features)\n",
      "    selector = SelectKBest(f_regression, k=nfeature)\n",
      "    selector.fit(X, y)\n",
      "    selected = selector.get_support()\n",
      "    feats = [col for (col,sel) in zip(predictors, selected) if sel]\n",
      "    reg = LinearRegression()\n",
      "    X_r = df[feats]\n",
      "    reg.fit(X_r, y)\n",
      "    ypred = reg.predict(X_r)\n",
      "    mses.append(np.sqrt(mean_squared_error(ypred, y)))\n",
      "\n",
      "plt.plot(nfeatures, mses)\n",
      "plt.xlabel(\"number of features\")\n",
      "plt.ylabel(\"RMSE\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "<matplotlib.text.Text at 0x7ff9ed4208d0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEPCAYAAABFpK+YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7ff9ed622290>"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Model selection by cross validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The RMSE falls as the number of features increase - this is expected because we are computing the RMSE off the training set (overfitting). We will now use 10-fold cross validation on each model to calculate a cross-validation MSE which will give us a better idea of the best feature size to use for the problem."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv_errors = []\n",
      "kfold = KFold(len(df), n_folds=10)\n",
      "nfeatures = range(1, len(predictors))\n",
      "\n",
      "for nfeature in nfeatures:\n",
      "    # build model with varying number of features\n",
      "    selector = SelectKBest(f_regression, k=nfeature)\n",
      "    selector.fit(X, y)\n",
      "    selected = selector.get_support()\n",
      "    feats = [col for (col,sel) in zip(predictors, selected) if sel]\n",
      "    X_r = df[feats].values\n",
      "    y = df[\"y\"].values\n",
      "    rmses = []\n",
      "    for train, test in kfold:\n",
      "        # each model is cross validated 10 times\n",
      "        Xtrain, ytrain, Xtest, ytest = X_r[train], y[train], X_r[test], y[test]   \n",
      "        reg = LinearRegression()\n",
      "        reg.fit(Xtrain, ytrain)\n",
      "        ypred = reg.predict(Xtest)\n",
      "        rmses.append(np.sqrt(mean_squared_error(ypred, ytest)))\n",
      "    cv_errors.append(np.mean(rmses))\n",
      "    \n",
      "plt.plot(nfeatures, cv_errors)\n",
      "plt.xlabel(\"number of features\")\n",
      "plt.ylabel(\"RMSE\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "<matplotlib.text.Text at 0x7ff9ed374c10>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEPCAYAAABFpK+YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiItMb/B+e079z/43hrAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7ff9ed645750>"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Compare baseline model with Ridge and Lasso"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cross_validate(X, y, nfolds, reg_name):\n",
      "    rmses = []\n",
      "    kfold = KFold(X.shape[0], n_folds=nfolds)\n",
      "    for train, test in kfold:\n",
      "        Xtrain, ytrain, Xtest, ytest = X[train], y[train], X[test], y[test]\n",
      "        reg = None\n",
      "        if reg_name == \"ridge\":\n",
      "            reg = Ridge()\n",
      "        elif reg_name == \"lasso\":\n",
      "            reg = Lasso()\n",
      "        else:\n",
      "            reg = LinearRegression()\n",
      "        reg.fit(Xtrain, ytrain)\n",
      "        ypred = reg.predict(Xtest)\n",
      "        rmses.append(np.sqrt(mean_squared_error(ytest, ypred)))\n",
      "    return np.mean(rmses)\n",
      "\n",
      "X = df[predictors].values\n",
      "y = df[\"y\"].values\n",
      "\n",
      "rmse_baseline = cross_validate(X, y, 10, \"baseline\")\n",
      "rmse_ridge = cross_validate(X, y, 10, \"ridge\")\n",
      "rmse_lasso = cross_validate(X, y, 10, \"lasso\")\n",
      "\n",
      "print(\"baseline wins with: %0.2f; ridge: %0.2f; lasso: %0.2f\") % (rmse_baseline, rmse_ridge, rmse_lasso)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "baseline wins with: 54.40; ridge: 57.75; lasso: 61.89\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Find tune the baseline model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "??? Not sure how to accomplish this, but I want to use SelectKBest where k = 5, and then apply cross validation to test for the optimal degree of the polynomial model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kfold = KFold(len(df), n_folds=10)\n",
      "nfeatures = [5]\n",
      "\n",
      "for nfeature in nfeatures:\n",
      "    # build model with varying number of features\n",
      "    selector = SelectKBest(f_regression, k=nfeature)\n",
      "    selector.fit(X, y)\n",
      "    selected = selector.get_support()\n",
      "    feats = [col for (col,sel) in zip(predictors, selected) if sel]\n",
      "    X_r = df[feats].values\n",
      "    y = df[\"y\"].values\n",
      "\n",
      "    degrees = np.arange(21)\n",
      "    train_err = np.zeros(len(degrees))\n",
      "    validation_err = np.zeros(len(degrees))\n",
      "\n",
      "    for i, d in enumerate(degrees):\n",
      "        for train, test in kfold:\n",
      "            # each model is cross validated 10 times\n",
      "            Xtrain, ytrain, Xtest, ytest = X_r[train], y[train], X_r[test], y[test]\n",
      "            \n",
      "            p = np.polyfit(Xtrain, ytrain, d)\n",
      "\n",
      "            train_err[i] = compute_error(Xtrain, ytrain, p)\n",
      "            validation_err[i] = compute_error(Xtest, ytest, p)\n",
      "            \n",
      "            \n",
      "fig, ax = plt.subplots()\n",
      "\n",
      "ax.plot(degrees, validation_err, lw=2, label = 'cross-validation error')\n",
      "ax.plot(degrees, train_err, lw=2, label = 'training error')\n",
      "\n",
      "ax.set_xlabel('degree of fit')\n",
      "ax.set_ylabel('rms error')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Found array with dim 7. Expected 442",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-58-24655e23b797>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# build model with varying number of features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_regression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mselected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/univariate_selection.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_arrays\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m             raise ValueError(\"Found array with dim %d. Expected %d\"\n\u001b[1;32m--> 254\u001b[1;33m                              % (size, n_samples))\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_lists\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: Found array with dim 7. Expected 442"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}