{
 "metadata": {
  "name": "",
  "signature": "sha256:c3821a8e26a22fef08aa8077a6765b695a7f00696ef8ad565fbe37d9c6fcf6eb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "Image(url=\"http://www.contemporaryartdaily.com/wp-content/uploads/2011/07/14-FromPoint3Panels.jpg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"http://www.contemporaryartdaily.com/wp-content/uploads/2011/07/14-FromPoint3Panels.jpg\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 338,
       "text": [
        "<IPython.core.display.Image at 0x97054d0>"
       ]
      }
     ],
     "prompt_number": 338
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "import numpy as np\n",
      "from itertools import product\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.metrics import accuracy_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv('https://raw.githubusercontent.com/TeachingDataScience/data-science-course/forstudentviewing/12_Naive_Bayes/twitter_training/sts_gold_tweet.csv',';')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "models = [MultinomialNB(), BernoulliNB(), LogisticRegression()]\n",
      "num_words = [int(x) for x in list(np.linspace(20,1000,20))]\n",
      "vectorizers = [CountVectorizer(ngram_range=(1,1), stop_words='english', max_features=words) for words in num_words]\n",
      "\n",
      "results ={}\n",
      "for model, vectorizer in product(models, vectorizers):\n",
      "    modelname = \"%s%d\"% (model.__class__.__name__, vectorizer.max_features)\n",
      "    kf = KFold(df.shape[0], 5)\n",
      "    cv_accuracies = []\n",
      "    for train, test in kf:\n",
      "        training_bag = vectorizer.fit_transform(df['tweet'][train])\n",
      "        testing_bag = vectorizer.transform(df['tweet'][test])\n",
      "        model.fit(training_bag, df['polarity'][train])\n",
      "        cv_accuracy = accuracy_score(model.predict(testing_bag), df['polarity'][test])\n",
      "        cv_accuracies.append(cv_accuracy)\n",
      "    results[modelname]={\n",
      "        'model': model.__class__.__name__,\n",
      "        'num_words': vectorizer.max_features,\n",
      "        'cv_accuracies': cv_accuracies,\n",
      "        'avg_accuracy': sum(cv_accuracies)/5\n",
      "    }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 341
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame.from_dict(results.values()).sort(['avg_accuracy'], ascending=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>avg_accuracy</th>\n",
        "      <th>cv_accuracies</th>\n",
        "      <th>model</th>\n",
        "      <th>num_words</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>59</th>\n",
        "      <td> 0.817615</td>\n",
        "      <td> [0.791154791155, 0.746928746929, 0.83538083538...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28</th>\n",
        "      <td> 0.815647</td>\n",
        "      <td> [0.791154791155, 0.742014742015, 0.83292383292...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  638</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td> 0.815647</td>\n",
        "      <td> [0.786240786241, 0.749385749386, 0.82800982801...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  535</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>35</th>\n",
        "      <td> 0.814666</td>\n",
        "      <td> [0.786240786241, 0.749385749386, 0.83292383292...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  948</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td> 0.813681</td>\n",
        "      <td> [0.796068796069, 0.746928746929, 0.82800982801...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  690</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>52</th>\n",
        "      <td> 0.813192</td>\n",
        "      <td> [0.786240786241, 0.744471744472, 0.83046683046...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  896</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>38</th>\n",
        "      <td> 0.813192</td>\n",
        "      <td> [0.793611793612, 0.746928746929, 0.82800982801...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  845</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>49</th>\n",
        "      <td> 0.812206</td>\n",
        "      <td> [0.793611793612, 0.746928746929, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  742</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td> 0.812206</td>\n",
        "      <td> [0.796068796069, 0.744471744472, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  793</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>33</th>\n",
        "      <td> 0.811714</td>\n",
        "      <td> [0.776412776413, 0.746928746929, 0.82309582309...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  484</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>44</th>\n",
        "      <td> 0.810735</td>\n",
        "      <td> [0.783783783784, 0.742014742015, 0.82555282555...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td> 1000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>32</th>\n",
        "      <td> 0.808284</td>\n",
        "      <td> [0.769041769042, 0.714987714988, 0.83046683046...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td> 0.808282</td>\n",
        "      <td> [0.771498771499, 0.719901719902, 0.82309582309...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  535</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td> 0.805812</td>\n",
        "      <td> [0.771498771499, 0.737100737101, 0.82555282555...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  432</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>39</th>\n",
        "      <td> 0.805330</td>\n",
        "      <td> [0.764127764128, 0.719901719902, 0.82063882063...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  484</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24</th>\n",
        "      <td> 0.804350</td>\n",
        "      <td> [0.759213759214, 0.712530712531, 0.82309582309...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td> 1000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> 0.804350</td>\n",
        "      <td> [0.766584766585, 0.697788697789, 0.82800982801...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  638</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>31</th>\n",
        "      <td> 0.802872</td>\n",
        "      <td> [0.7542997543, 0.707616707617, 0.82800982801, ...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  845</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> 0.802381</td>\n",
        "      <td> [0.756756756757, 0.710073710074, 0.82309582309...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  948</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> 0.802370</td>\n",
        "      <td> [0.769041769042, 0.734643734644, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  381</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>51</th>\n",
        "      <td> 0.801893</td>\n",
        "      <td> [0.759213759214, 0.692874692875, 0.83046683046...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  690</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> 0.801888</td>\n",
        "      <td> [0.7542997543, 0.719901719902, 0.820638820639,...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  432</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>37</th>\n",
        "      <td> 0.801888</td>\n",
        "      <td> [0.756756756757, 0.710073710074, 0.82309582309...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  896</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 0.801887</td>\n",
        "      <td> [0.761670761671, 0.707616707617, 0.82800982801...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  793</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>41</th>\n",
        "      <td> 0.801394</td>\n",
        "      <td> [0.761670761671, 0.727272727273, 0.81081081081...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  381</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td> 0.800420</td>\n",
        "      <td> [0.751842751843, 0.687960687961, 0.83046683046...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  793</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>54</th>\n",
        "      <td> 0.800410</td>\n",
        "      <td> [0.759213759214, 0.734643734644, 0.80835380835...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  277</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>42</th>\n",
        "      <td> 0.799929</td>\n",
        "      <td> [0.751842751843, 0.692874692875, 0.82800982801...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  742</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>30</th>\n",
        "      <td> 0.798440</td>\n",
        "      <td> [0.756756756757, 0.744471744472, 0.80589680589...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  329</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> 0.797964</td>\n",
        "      <td> [0.756756756757, 0.685503685504, 0.82063882063...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  845</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>56</th>\n",
        "      <td> 0.797955</td>\n",
        "      <td> [0.751842751843, 0.714987714988, 0.82309582309...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  638</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23</th>\n",
        "      <td> 0.797464</td>\n",
        "      <td> [0.751842751843, 0.702702702703, 0.82555282555...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  742</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td> 0.796973</td>\n",
        "      <td> [0.744471744472, 0.710073710074, 0.82800982801...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  535</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>53</th>\n",
        "      <td> 0.796483</td>\n",
        "      <td> [0.751842751843, 0.72972972973, 0.798525798526...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  329</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>48</th>\n",
        "      <td> 0.796480</td>\n",
        "      <td> [0.761670761671, 0.727272727273, 0.79852579852...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  277</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50</th>\n",
        "      <td> 0.795991</td>\n",
        "      <td> [0.751842751843, 0.707616707617, 0.81572481572...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  690</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>58</th>\n",
        "      <td> 0.795990</td>\n",
        "      <td> [0.751842751843, 0.707616707617, 0.82555282555...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>46</th>\n",
        "      <td> 0.795510</td>\n",
        "      <td> [0.734643734644, 0.687960687961, 0.81818181818...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td> 1000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td> 0.795005</td>\n",
        "      <td> [0.751842751843, 0.727272727273, 0.80098280098...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  226</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td> 0.794514</td>\n",
        "      <td> [0.751842751843, 0.70515970516, 0.813267813268...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  381</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td> 0.794511</td>\n",
        "      <td> [0.7542997543, 0.710073710074, 0.825552825553,...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  277</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>47</th>\n",
        "      <td> 0.794022</td>\n",
        "      <td> [0.756756756757, 0.70515970516, 0.813267813268...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  329</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>55</th>\n",
        "      <td> 0.793543</td>\n",
        "      <td> [0.734643734644, 0.678132678133, 0.81818181818...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  948</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26</th>\n",
        "      <td> 0.793542</td>\n",
        "      <td> [0.737100737101, 0.68058968059, 0.818181818182...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  896</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>34</th>\n",
        "      <td> 0.792056</td>\n",
        "      <td> [0.732186732187, 0.717444717445, 0.81326781326...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  484</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27</th>\n",
        "      <td> 0.792053</td>\n",
        "      <td> [0.746928746929, 0.712530712531, 0.81572481572...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  432</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>40</th>\n",
        "      <td> 0.790088</td>\n",
        "      <td> [0.749385749386, 0.697788697789, 0.82309582309...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  226</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> 0.789602</td>\n",
        "      <td> [0.749385749386, 0.710073710074, 0.79606879606...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  226</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td> 0.781735</td>\n",
        "      <td> [0.734643734644, 0.697788697789, 0.79606879606...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  174</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> 0.779275</td>\n",
        "      <td> [0.722358722359, 0.683046683047, 0.80835380835...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  174</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>45</th>\n",
        "      <td> 0.778295</td>\n",
        "      <td> [0.737100737101, 0.695331695332, 0.79115479115...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  174</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td> 0.777313</td>\n",
        "      <td> [0.737100737101, 0.68058968059, 0.798525798526...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  123</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>36</th>\n",
        "      <td> 0.776327</td>\n",
        "      <td> [0.724815724816, 0.685503685504, 0.80098280098...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  123</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>43</th>\n",
        "      <td> 0.770921</td>\n",
        "      <td> [0.714987714988, 0.646191646192, 0.82309582309...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  123</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25</th>\n",
        "      <td> 0.748321</td>\n",
        "      <td> [0.673218673219, 0.628992628993, 0.79361179361...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>   71</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td> 0.746845</td>\n",
        "      <td> [0.665847665848, 0.621621621622, 0.79361179361...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>   71</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29</th>\n",
        "      <td> 0.744391</td>\n",
        "      <td> [0.673218673219, 0.597051597052, 0.79852579852...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>   71</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>57</th>\n",
        "      <td> 0.707087</td>\n",
        "      <td> [0.599508599509, 0.484029484029, 0.75921375921...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>   20</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> 0.703643</td>\n",
        "      <td> [0.592137592138, 0.488943488943, 0.75675675675...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>   20</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td> 0.701678</td>\n",
        "      <td> [0.609336609337, 0.464373464373, 0.74938574938...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>   20</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 342,
       "text": [
        "    avg_accuracy                                      cv_accuracies  \\\n",
        "59      0.817615  [0.791154791155, 0.746928746929, 0.83538083538...   \n",
        "28      0.815647  [0.791154791155, 0.742014742015, 0.83292383292...   \n",
        "22      0.815647  [0.786240786241, 0.749385749386, 0.82800982801...   \n",
        "35      0.814666  [0.786240786241, 0.749385749386, 0.83292383292...   \n",
        "15      0.813681  [0.796068796069, 0.746928746929, 0.82800982801...   \n",
        "52      0.813192  [0.786240786241, 0.744471744472, 0.83046683046...   \n",
        "38      0.813192  [0.793611793612, 0.746928746929, 0.82800982801...   \n",
        "49      0.812206  [0.793611793612, 0.746928746929, 0.82063882063...   \n",
        "4       0.812206  [0.796068796069, 0.744471744472, 0.82063882063...   \n",
        "33      0.811714  [0.776412776413, 0.746928746929, 0.82309582309...   \n",
        "44      0.810735  [0.783783783784, 0.742014742015, 0.82555282555...   \n",
        "32      0.808284  [0.769041769042, 0.714987714988, 0.83046683046...   \n",
        "20      0.808282  [0.771498771499, 0.719901719902, 0.82309582309...   \n",
        "17      0.805812  [0.771498771499, 0.737100737101, 0.82555282555...   \n",
        "39      0.805330  [0.764127764128, 0.719901719902, 0.82063882063...   \n",
        "24      0.804350  [0.759213759214, 0.712530712531, 0.82309582309...   \n",
        "13      0.804350  [0.766584766585, 0.697788697789, 0.82800982801...   \n",
        "31      0.802872  [0.7542997543, 0.707616707617, 0.82800982801, ...   \n",
        "11      0.802381  [0.756756756757, 0.710073710074, 0.82309582309...   \n",
        "5       0.802370  [0.769041769042, 0.734643734644, 0.82063882063...   \n",
        "51      0.801893  [0.759213759214, 0.692874692875, 0.83046683046...   \n",
        "10      0.801888  [0.7542997543, 0.719901719902, 0.820638820639,...   \n",
        "37      0.801888  [0.756756756757, 0.710073710074, 0.82309582309...   \n",
        "9       0.801887  [0.761670761671, 0.707616707617, 0.82800982801...   \n",
        "41      0.801394  [0.761670761671, 0.727272727273, 0.81081081081...   \n",
        "16      0.800420  [0.751842751843, 0.687960687961, 0.83046683046...   \n",
        "54      0.800410  [0.759213759214, 0.734643734644, 0.80835380835...   \n",
        "42      0.799929  [0.751842751843, 0.692874692875, 0.82800982801...   \n",
        "30      0.798440  [0.756756756757, 0.744471744472, 0.80589680589...   \n",
        "2       0.797964  [0.756756756757, 0.685503685504, 0.82063882063...   \n",
        "56      0.797955  [0.751842751843, 0.714987714988, 0.82309582309...   \n",
        "23      0.797464  [0.751842751843, 0.702702702703, 0.82555282555...   \n",
        "18      0.796973  [0.744471744472, 0.710073710074, 0.82800982801...   \n",
        "53      0.796483  [0.751842751843, 0.72972972973, 0.798525798526...   \n",
        "48      0.796480  [0.761670761671, 0.727272727273, 0.79852579852...   \n",
        "50      0.795991  [0.751842751843, 0.707616707617, 0.81572481572...   \n",
        "58      0.795990  [0.751842751843, 0.707616707617, 0.82555282555...   \n",
        "46      0.795510  [0.734643734644, 0.687960687961, 0.81818181818...   \n",
        "0       0.795005  [0.751842751843, 0.727272727273, 0.80098280098...   \n",
        "3       0.794514  [0.751842751843, 0.70515970516, 0.813267813268...   \n",
        "14      0.794511  [0.7542997543, 0.710073710074, 0.825552825553,...   \n",
        "47      0.794022  [0.756756756757, 0.70515970516, 0.813267813268...   \n",
        "55      0.793543  [0.734643734644, 0.678132678133, 0.81818181818...   \n",
        "26      0.793542  [0.737100737101, 0.68058968059, 0.818181818182...   \n",
        "34      0.792056  [0.732186732187, 0.717444717445, 0.81326781326...   \n",
        "27      0.792053  [0.746928746929, 0.712530712531, 0.81572481572...   \n",
        "40      0.790088  [0.749385749386, 0.697788697789, 0.82309582309...   \n",
        "12      0.789602  [0.749385749386, 0.710073710074, 0.79606879606...   \n",
        "1       0.781735  [0.734643734644, 0.697788697789, 0.79606879606...   \n",
        "7       0.779275  [0.722358722359, 0.683046683047, 0.80835380835...   \n",
        "45      0.778295  [0.737100737101, 0.695331695332, 0.79115479115...   \n",
        "19      0.777313  [0.737100737101, 0.68058968059, 0.798525798526...   \n",
        "36      0.776327  [0.724815724816, 0.685503685504, 0.80098280098...   \n",
        "43      0.770921  [0.714987714988, 0.646191646192, 0.82309582309...   \n",
        "25      0.748321  [0.673218673219, 0.628992628993, 0.79361179361...   \n",
        "6       0.746845  [0.665847665848, 0.621621621622, 0.79361179361...   \n",
        "29      0.744391  [0.673218673219, 0.597051597052, 0.79852579852...   \n",
        "57      0.707087  [0.599508599509, 0.484029484029, 0.75921375921...   \n",
        "8       0.703643  [0.592137592138, 0.488943488943, 0.75675675675...   \n",
        "21      0.701678  [0.609336609337, 0.464373464373, 0.74938574938...   \n",
        "\n",
        "                 model  num_words  \n",
        "59       MultinomialNB        587  \n",
        "28       MultinomialNB        638  \n",
        "22       MultinomialNB        535  \n",
        "35       MultinomialNB        948  \n",
        "15       MultinomialNB        690  \n",
        "52       MultinomialNB        896  \n",
        "38       MultinomialNB        845  \n",
        "49       MultinomialNB        742  \n",
        "4        MultinomialNB        793  \n",
        "33       MultinomialNB        484  \n",
        "44       MultinomialNB       1000  \n",
        "32         BernoulliNB        587  \n",
        "20         BernoulliNB        535  \n",
        "17       MultinomialNB        432  \n",
        "39         BernoulliNB        484  \n",
        "24  LogisticRegression       1000  \n",
        "13         BernoulliNB        638  \n",
        "31  LogisticRegression        845  \n",
        "11  LogisticRegression        948  \n",
        "5        MultinomialNB        381  \n",
        "51         BernoulliNB        690  \n",
        "10         BernoulliNB        432  \n",
        "37  LogisticRegression        896  \n",
        "9   LogisticRegression        793  \n",
        "41         BernoulliNB        381  \n",
        "16         BernoulliNB        793  \n",
        "54       MultinomialNB        277  \n",
        "42         BernoulliNB        742  \n",
        "30       MultinomialNB        329  \n",
        "2          BernoulliNB        845  \n",
        "56  LogisticRegression        638  \n",
        "23  LogisticRegression        742  \n",
        "18  LogisticRegression        535  \n",
        "53         BernoulliNB        329  \n",
        "48         BernoulliNB        277  \n",
        "50  LogisticRegression        690  \n",
        "58  LogisticRegression        587  \n",
        "46         BernoulliNB       1000  \n",
        "0        MultinomialNB        226  \n",
        "3   LogisticRegression        381  \n",
        "14  LogisticRegression        277  \n",
        "47  LogisticRegression        329  \n",
        "55         BernoulliNB        948  \n",
        "26         BernoulliNB        896  \n",
        "34  LogisticRegression        484  \n",
        "27  LogisticRegression        432  \n",
        "40  LogisticRegression        226  \n",
        "12         BernoulliNB        226  \n",
        "1        MultinomialNB        174  \n",
        "7   LogisticRegression        174  \n",
        "45         BernoulliNB        174  \n",
        "19         BernoulliNB        123  \n",
        "36       MultinomialNB        123  \n",
        "43  LogisticRegression        123  \n",
        "25         BernoulliNB         71  \n",
        "6        MultinomialNB         71  \n",
        "29  LogisticRegression         71  \n",
        "57       MultinomialNB         20  \n",
        "8          BernoulliNB         20  \n",
        "21  LogisticRegression         20  "
       ]
      }
     ],
     "prompt_number": 342
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Adjust the above to find optimal ngram"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "models = [MultinomialNB(), BernoulliNB(), LogisticRegression()]\n",
      "ngram_ranges = [int(x) for x in list(np.linspace(1,10,10))]\n",
      "vectorizers = [CountVectorizer(ngram_range=(1,ngram), stop_words='english', max_features=587) for ngram in ngram_ranges]\n",
      "num_folds = 5\n",
      "\n",
      "results ={}\n",
      "for model, vectorizer in product(models, vectorizers):\n",
      "    modelname = \"%s%d\"% (model.__class__.__name__, vectorizer.ngram_range[1])\n",
      "    kf = KFold(df.shape[0], num_folds)\n",
      "    cv_accuracies = []\n",
      "    for train, test in kf:\n",
      "        training_bag = vectorizer.fit_transform(df['tweet'][train])\n",
      "        testing_bag = vectorizer.transform(df['tweet'][test])\n",
      "        model.fit(training_bag, df['polarity'][train])\n",
      "        cv_accuracy = accuracy_score(model.predict(testing_bag), df['polarity'][test])\n",
      "        cv_accuracies.append(cv_accuracy)\n",
      "    results[modelname]={\n",
      "        'model': model.__class__.__name__,\n",
      "        'ngram_range': vectorizer.ngram_range[1],\n",
      "        'cv_accuracies': cv_accuracies,\n",
      "        'avg_accuracy': sum(cv_accuracies)/num_folds\n",
      "    }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame.from_dict(results.values()).sort(['avg_accuracy'], ascending=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>avg_accuracy</th>\n",
        "      <th>cv_accuracies</th>\n",
        "      <th>model</th>\n",
        "      <th>num_words</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td> 0.817615</td>\n",
        "      <td> [0.791154791155, 0.746928746929, 0.83538083538...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  (1, 1)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td> 0.812205</td>\n",
        "      <td> [0.788697788698, 0.742014742015, 0.82800982801...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  (1, 2)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td> 0.812204</td>\n",
        "      <td> [0.786240786241, 0.744471744472, 0.83046683046...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  (1, 3)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td> 0.811716</td>\n",
        "      <td> [0.788697788698, 0.737100737101, 0.82555282555...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  (1, 4)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td> 0.811710</td>\n",
        "      <td> [0.786240786241, 0.749385749386, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  (1, 5)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td> 0.811224</td>\n",
        "      <td> [0.781326781327, 0.749385749386, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  (1, 6)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25</th>\n",
        "      <td> 0.808284</td>\n",
        "      <td> [0.769041769042, 0.714987714988, 0.83046683046...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  (1, 1)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> 0.808273</td>\n",
        "      <td> [0.786240786241, 0.739557739558, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td> (1, 10)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td> 0.807780</td>\n",
        "      <td> [0.783783783784, 0.742014742015, 0.81326781326...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  (1, 7)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> 0.807287</td>\n",
        "      <td> [0.781326781327, 0.744471744472, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  (1, 8)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> 0.806799</td>\n",
        "      <td> [0.776412776413, 0.742014742015, 0.80835380835...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  (1, 9)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24</th>\n",
        "      <td> 0.802875</td>\n",
        "      <td> [0.759213759214, 0.717444717445, 0.81572481572...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  (1, 7)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> 0.802875</td>\n",
        "      <td> [0.766584766585, 0.712530712531, 0.81572481572...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td> (1, 10)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26</th>\n",
        "      <td> 0.802380</td>\n",
        "      <td> [0.756756756757, 0.717444717445, 0.82309582309...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  (1, 2)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27</th>\n",
        "      <td> 0.801892</td>\n",
        "      <td> [0.761670761671, 0.70515970516, 0.820638820639...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  (1, 3)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td> 0.801398</td>\n",
        "      <td> [0.759213759214, 0.710073710074, 0.81818181818...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  (1, 5)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23</th>\n",
        "      <td> 0.800908</td>\n",
        "      <td> [0.749385749386, 0.719901719902, 0.81572481572...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  (1, 6)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td> 0.800908</td>\n",
        "      <td> [0.761670761671, 0.707616707617, 0.82063882063...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  (1, 4)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28</th>\n",
        "      <td> 0.799434</td>\n",
        "      <td> [0.746928746929, 0.717444717445, 0.81326781326...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  (1, 8)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29</th>\n",
        "      <td> 0.797959</td>\n",
        "      <td> [0.756756756757, 0.700245700246, 0.81326781326...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  (1, 9)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td> 0.795990</td>\n",
        "      <td> [0.751842751843, 0.707616707617, 0.82555282555...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  (1, 1)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td> 0.795010</td>\n",
        "      <td> [0.746928746929, 0.714987714988, 0.81326781326...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  (1, 4)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td> 0.795005</td>\n",
        "      <td> [0.744471744472, 0.717444717445, 0.81818181818...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  (1, 7)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> 0.795004</td>\n",
        "      <td> [0.744471744472, 0.714987714988, 0.82063882063...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  (1, 8)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> 0.794517</td>\n",
        "      <td> [0.739557739558, 0.712530712531, 0.81326781326...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  (1, 9)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> 0.794023</td>\n",
        "      <td> [0.744471744472, 0.717444717445, 0.81326781326...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  (1, 3)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td> 0.793533</td>\n",
        "      <td> [0.739557739558, 0.710073710074, 0.82063882063...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  (1, 2)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td> 0.793038</td>\n",
        "      <td> [0.744471744472, 0.710073710074, 0.81572481572...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  (1, 5)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> 0.792550</td>\n",
        "      <td> [0.739557739558, 0.712530712531, 0.81081081081...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  (1, 6)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 0.792544</td>\n",
        "      <td> [0.742014742015, 0.712530712531, 0.81818181818...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td> (1, 10)</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "    avg_accuracy                                      cv_accuracies  \\\n",
        "16      0.817615  [0.791154791155, 0.746928746929, 0.83538083538...   \n",
        "15      0.812205  [0.788697788698, 0.742014742015, 0.82800982801...   \n",
        "14      0.812204  [0.786240786241, 0.744471744472, 0.83046683046...   \n",
        "20      0.811716  [0.788697788698, 0.737100737101, 0.82555282555...   \n",
        "19      0.811710  [0.786240786241, 0.749385749386, 0.82063882063...   \n",
        "18      0.811224  [0.781326781327, 0.749385749386, 0.82063882063...   \n",
        "25      0.808284  [0.769041769042, 0.714987714988, 0.83046683046...   \n",
        "10      0.808273  [0.786240786241, 0.739557739558, 0.82063882063...   \n",
        "17      0.807780  [0.783783783784, 0.742014742015, 0.81326781326...   \n",
        "13      0.807287  [0.781326781327, 0.744471744472, 0.82063882063...   \n",
        "12      0.806799  [0.776412776413, 0.742014742015, 0.80835380835...   \n",
        "24      0.802875  [0.759213759214, 0.717444717445, 0.81572481572...   \n",
        "11      0.802875  [0.766584766585, 0.712530712531, 0.81572481572...   \n",
        "26      0.802380  [0.756756756757, 0.717444717445, 0.82309582309...   \n",
        "27      0.801892  [0.761670761671, 0.70515970516, 0.820638820639...   \n",
        "22      0.801398  [0.759213759214, 0.710073710074, 0.81818181818...   \n",
        "23      0.800908  [0.749385749386, 0.719901719902, 0.81572481572...   \n",
        "21      0.800908  [0.761670761671, 0.707616707617, 0.82063882063...   \n",
        "28      0.799434  [0.746928746929, 0.717444717445, 0.81326781326...   \n",
        "29      0.797959  [0.756756756757, 0.700245700246, 0.81326781326...   \n",
        "0       0.795990  [0.751842751843, 0.707616707617, 0.82555282555...   \n",
        "3       0.795010  [0.746928746929, 0.714987714988, 0.81326781326...   \n",
        "6       0.795005  [0.744471744472, 0.717444717445, 0.81818181818...   \n",
        "7       0.795004  [0.744471744472, 0.714987714988, 0.82063882063...   \n",
        "8       0.794517  [0.739557739558, 0.712530712531, 0.81326781326...   \n",
        "2       0.794023  [0.744471744472, 0.717444717445, 0.81326781326...   \n",
        "1       0.793533  [0.739557739558, 0.710073710074, 0.82063882063...   \n",
        "4       0.793038  [0.744471744472, 0.710073710074, 0.81572481572...   \n",
        "5       0.792550  [0.739557739558, 0.712530712531, 0.81081081081...   \n",
        "9       0.792544  [0.742014742015, 0.712530712531, 0.81818181818...   \n",
        "\n",
        "                 model num_words  \n",
        "16       MultinomialNB    (1, 1)  \n",
        "15       MultinomialNB    (1, 2)  \n",
        "14       MultinomialNB    (1, 3)  \n",
        "20       MultinomialNB    (1, 4)  \n",
        "19       MultinomialNB    (1, 5)  \n",
        "18       MultinomialNB    (1, 6)  \n",
        "25         BernoulliNB    (1, 1)  \n",
        "10       MultinomialNB   (1, 10)  \n",
        "17       MultinomialNB    (1, 7)  \n",
        "13       MultinomialNB    (1, 8)  \n",
        "12       MultinomialNB    (1, 9)  \n",
        "24         BernoulliNB    (1, 7)  \n",
        "11         BernoulliNB   (1, 10)  \n",
        "26         BernoulliNB    (1, 2)  \n",
        "27         BernoulliNB    (1, 3)  \n",
        "22         BernoulliNB    (1, 5)  \n",
        "23         BernoulliNB    (1, 6)  \n",
        "21         BernoulliNB    (1, 4)  \n",
        "28         BernoulliNB    (1, 8)  \n",
        "29         BernoulliNB    (1, 9)  \n",
        "0   LogisticRegression    (1, 1)  \n",
        "3   LogisticRegression    (1, 4)  \n",
        "6   LogisticRegression    (1, 7)  \n",
        "7   LogisticRegression    (1, 8)  \n",
        "8   LogisticRegression    (1, 9)  \n",
        "2   LogisticRegression    (1, 3)  \n",
        "1   LogisticRegression    (1, 2)  \n",
        "4   LogisticRegression    (1, 5)  \n",
        "5   LogisticRegression    (1, 6)  \n",
        "9   LogisticRegression   (1, 10)  "
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Change the accuracy score to something else"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import precision_score\n",
      "\n",
      "models = [MultinomialNB(), BernoulliNB(), LogisticRegression()]\n",
      "ngram_ranges = [int(x) for x in list(np.linspace(1,10,10))]\n",
      "vectorizers = [CountVectorizer(ngram_range=(1,ngram), stop_words='english', max_features=587) for ngram in ngram_ranges]\n",
      "num_folds = 5\n",
      "\n",
      "results ={}\n",
      "for model, vectorizer in product(models, vectorizers):\n",
      "    modelname = \"%s%d\"% (model.__class__.__name__, vectorizer.ngram_range[1])\n",
      "    kf = KFold(df.shape[0], num_folds) \n",
      "    cv_accuracies = []\n",
      "    for train, test in kf:\n",
      "        training_bag = vectorizer.fit_transform(df['tweet'][train])\n",
      "        testing_bag = vectorizer.transform(df['tweet'][test])\n",
      "        model.fit(training_bag, df['polarity'][train])\n",
      "        cv_accuracy = precision_score(model.predict(testing_bag), df['polarity'][test])\n",
      "        cv_accuracies.append(cv_accuracy)\n",
      "    results[modelname]={\n",
      "        'model': model.__class__.__name__,\n",
      "        'ngram_range': vectorizer.ngram_range[1],\n",
      "        'cv_accuracies': cv_accuracies,\n",
      "        'avg_accuracy': sum(cv_accuracies)/num_folds\n",
      "    }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "pos_label=1 is not a valid label: array([0, 4])",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-18-50ad6224c9d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtesting_bag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_bag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'polarity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mcv_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_bag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'polarity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mcv_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     results[modelname]={\n",
        "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/metrics/metrics.pyc\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1883\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1885\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1886\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/metrics/metrics.pyc\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m                 raise ValueError(\"pos_label=%r is not a valid label: %r\" %\n\u001b[0;32m-> 1751\u001b[0;31m                                  (pos_label, labels))\n\u001b[0m\u001b[1;32m   1752\u001b[0m         \u001b[0mpos_label_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m         \u001b[0mtp_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_label_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: pos_label=1 is not a valid label: array([0, 4])"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame.from_dict(results.values()).sort(['avg_accuracy'], ascending=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>avg_accuracy</th>\n",
        "      <th>cv_accuracies</th>\n",
        "      <th>model</th>\n",
        "      <th>ngram_range</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td> 0.817615</td>\n",
        "      <td> [0.791154791155, 0.746928746929, 0.83538083538...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td> 0.812205</td>\n",
        "      <td> [0.788697788698, 0.742014742015, 0.82800982801...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td> 0.812204</td>\n",
        "      <td> [0.786240786241, 0.744471744472, 0.83046683046...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td> 0.811716</td>\n",
        "      <td> [0.788697788698, 0.737100737101, 0.82555282555...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td> 0.811710</td>\n",
        "      <td> [0.786240786241, 0.749385749386, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td> 0.811224</td>\n",
        "      <td> [0.781326781327, 0.749385749386, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  6</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25</th>\n",
        "      <td> 0.808284</td>\n",
        "      <td> [0.769041769042, 0.714987714988, 0.83046683046...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> 0.808273</td>\n",
        "      <td> [0.786240786241, 0.739557739558, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td> 10</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td> 0.807780</td>\n",
        "      <td> [0.783783783784, 0.742014742015, 0.81326781326...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> 0.807287</td>\n",
        "      <td> [0.781326781327, 0.744471744472, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  8</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> 0.806799</td>\n",
        "      <td> [0.776412776413, 0.742014742015, 0.80835380835...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  9</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24</th>\n",
        "      <td> 0.802875</td>\n",
        "      <td> [0.759213759214, 0.717444717445, 0.81572481572...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> 0.802875</td>\n",
        "      <td> [0.766584766585, 0.712530712531, 0.81572481572...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td> 10</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26</th>\n",
        "      <td> 0.802380</td>\n",
        "      <td> [0.756756756757, 0.717444717445, 0.82309582309...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27</th>\n",
        "      <td> 0.801892</td>\n",
        "      <td> [0.761670761671, 0.70515970516, 0.820638820639...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td> 0.801398</td>\n",
        "      <td> [0.759213759214, 0.710073710074, 0.81818181818...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23</th>\n",
        "      <td> 0.800908</td>\n",
        "      <td> [0.749385749386, 0.719901719902, 0.81572481572...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  6</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td> 0.800908</td>\n",
        "      <td> [0.761670761671, 0.707616707617, 0.82063882063...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28</th>\n",
        "      <td> 0.799434</td>\n",
        "      <td> [0.746928746929, 0.717444717445, 0.81326781326...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  8</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29</th>\n",
        "      <td> 0.797959</td>\n",
        "      <td> [0.756756756757, 0.700245700246, 0.81326781326...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  9</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td> 0.795990</td>\n",
        "      <td> [0.751842751843, 0.707616707617, 0.82555282555...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td> 0.795010</td>\n",
        "      <td> [0.746928746929, 0.714987714988, 0.81326781326...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td> 0.795005</td>\n",
        "      <td> [0.744471744472, 0.717444717445, 0.81818181818...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> 0.795004</td>\n",
        "      <td> [0.744471744472, 0.714987714988, 0.82063882063...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  8</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> 0.794517</td>\n",
        "      <td> [0.739557739558, 0.712530712531, 0.81326781326...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  9</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> 0.794023</td>\n",
        "      <td> [0.744471744472, 0.717444717445, 0.81326781326...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td> 0.793533</td>\n",
        "      <td> [0.739557739558, 0.710073710074, 0.82063882063...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td> 0.793038</td>\n",
        "      <td> [0.744471744472, 0.710073710074, 0.81572481572...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> 0.792550</td>\n",
        "      <td> [0.739557739558, 0.712530712531, 0.81081081081...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  6</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 0.792544</td>\n",
        "      <td> [0.742014742015, 0.712530712531, 0.81818181818...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td> 10</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "    avg_accuracy                                      cv_accuracies  \\\n",
        "16      0.817615  [0.791154791155, 0.746928746929, 0.83538083538...   \n",
        "15      0.812205  [0.788697788698, 0.742014742015, 0.82800982801...   \n",
        "14      0.812204  [0.786240786241, 0.744471744472, 0.83046683046...   \n",
        "20      0.811716  [0.788697788698, 0.737100737101, 0.82555282555...   \n",
        "19      0.811710  [0.786240786241, 0.749385749386, 0.82063882063...   \n",
        "18      0.811224  [0.781326781327, 0.749385749386, 0.82063882063...   \n",
        "25      0.808284  [0.769041769042, 0.714987714988, 0.83046683046...   \n",
        "10      0.808273  [0.786240786241, 0.739557739558, 0.82063882063...   \n",
        "17      0.807780  [0.783783783784, 0.742014742015, 0.81326781326...   \n",
        "13      0.807287  [0.781326781327, 0.744471744472, 0.82063882063...   \n",
        "12      0.806799  [0.776412776413, 0.742014742015, 0.80835380835...   \n",
        "24      0.802875  [0.759213759214, 0.717444717445, 0.81572481572...   \n",
        "11      0.802875  [0.766584766585, 0.712530712531, 0.81572481572...   \n",
        "26      0.802380  [0.756756756757, 0.717444717445, 0.82309582309...   \n",
        "27      0.801892  [0.761670761671, 0.70515970516, 0.820638820639...   \n",
        "22      0.801398  [0.759213759214, 0.710073710074, 0.81818181818...   \n",
        "23      0.800908  [0.749385749386, 0.719901719902, 0.81572481572...   \n",
        "21      0.800908  [0.761670761671, 0.707616707617, 0.82063882063...   \n",
        "28      0.799434  [0.746928746929, 0.717444717445, 0.81326781326...   \n",
        "29      0.797959  [0.756756756757, 0.700245700246, 0.81326781326...   \n",
        "0       0.795990  [0.751842751843, 0.707616707617, 0.82555282555...   \n",
        "3       0.795010  [0.746928746929, 0.714987714988, 0.81326781326...   \n",
        "6       0.795005  [0.744471744472, 0.717444717445, 0.81818181818...   \n",
        "7       0.795004  [0.744471744472, 0.714987714988, 0.82063882063...   \n",
        "8       0.794517  [0.739557739558, 0.712530712531, 0.81326781326...   \n",
        "2       0.794023  [0.744471744472, 0.717444717445, 0.81326781326...   \n",
        "1       0.793533  [0.739557739558, 0.710073710074, 0.82063882063...   \n",
        "4       0.793038  [0.744471744472, 0.710073710074, 0.81572481572...   \n",
        "5       0.792550  [0.739557739558, 0.712530712531, 0.81081081081...   \n",
        "9       0.792544  [0.742014742015, 0.712530712531, 0.81818181818...   \n",
        "\n",
        "                 model  ngram_range  \n",
        "16       MultinomialNB            1  \n",
        "15       MultinomialNB            2  \n",
        "14       MultinomialNB            3  \n",
        "20       MultinomialNB            4  \n",
        "19       MultinomialNB            5  \n",
        "18       MultinomialNB            6  \n",
        "25         BernoulliNB            1  \n",
        "10       MultinomialNB           10  \n",
        "17       MultinomialNB            7  \n",
        "13       MultinomialNB            8  \n",
        "12       MultinomialNB            9  \n",
        "24         BernoulliNB            7  \n",
        "11         BernoulliNB           10  \n",
        "26         BernoulliNB            2  \n",
        "27         BernoulliNB            3  \n",
        "22         BernoulliNB            5  \n",
        "23         BernoulliNB            6  \n",
        "21         BernoulliNB            4  \n",
        "28         BernoulliNB            8  \n",
        "29         BernoulliNB            9  \n",
        "0   LogisticRegression            1  \n",
        "3   LogisticRegression            4  \n",
        "6   LogisticRegression            7  \n",
        "7   LogisticRegression            8  \n",
        "8   LogisticRegression            9  \n",
        "2   LogisticRegression            3  \n",
        "1   LogisticRegression            2  \n",
        "4   LogisticRegression            5  \n",
        "5   LogisticRegression            6  \n",
        "9   LogisticRegression           10  "
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Adding additional predictors like 'percent capitalized'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>id</th>\n",
        "      <th>polarity</th>\n",
        "      <th>tweet</th>\n",
        "      <th>percent_capitalized</th>\n",
        "      <th>exclamation</th>\n",
        "      <th>at</th>\n",
        "      <th>link</th>\n",
        "      <th>polarity_hat</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1467933112</td>\n",
        "      <td> 0</td>\n",
        "      <td> the angel is going to miss the athlete this we...</td>\n",
        "      <td> 0.192308</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 2323395086</td>\n",
        "      <td> 0</td>\n",
        "      <td> It looks as though Shaq is getting traded to C...</td>\n",
        "      <td> 0.325397</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 1467968979</td>\n",
        "      <td> 0</td>\n",
        "      <td>    @clarianne APRIL 9TH ISN'T COMING SOON ENOUGH </td>\n",
        "      <td> 0.804348</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 1990283756</td>\n",
        "      <td> 0</td>\n",
        "      <td> drinking a McDonalds coffee and not understand...</td>\n",
        "      <td> 0.190000</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 1988884918</td>\n",
        "      <td> 0</td>\n",
        "      <td> So dissapointed Taylor Swift doesnt have a Twi...</td>\n",
        "      <td> 0.235294</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0</td>\n",
        "      <td> 4</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "           id  polarity                                              tweet  \\\n",
        "0  1467933112         0  the angel is going to miss the athlete this we...   \n",
        "1  2323395086         0  It looks as though Shaq is getting traded to C...   \n",
        "2  1467968979         0     @clarianne APRIL 9TH ISN'T COMING SOON ENOUGH    \n",
        "3  1990283756         0  drinking a McDonalds coffee and not understand...   \n",
        "4  1988884918         0  So dissapointed Taylor Swift doesnt have a Twi...   \n",
        "\n",
        "   percent_capitalized  exclamation  at  link  polarity_hat  \n",
        "0             0.192308            0   0     0             0  \n",
        "1             0.325397            0   0     0             0  \n",
        "2             0.804348            0   1     0             0  \n",
        "3             0.190000            0   0     0             0  \n",
        "4             0.235294            0   0     0             4  "
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['percent_capitalized'] = df.tweet.apply(lambda x: sum([float(x[i] == x.upper()[i]) for i in range(len(x))])/len(x))\n",
      "df['exclamation'] = df.tweet.apply(lambda x: 1 if \"!\" in x else 0)\n",
      "df['at'] = df.tweet.apply(lambda x: 1 if \"@\" in x else 0)\n",
      "df['link'] = df.tweet.apply(lambda x: 1 if \"http\" in x else 0)\n",
      "\n",
      "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words='english', max_features=587)\n",
      "bag_of_words = vectorizer.fit_transform(df['tweet'])\n",
      "\n",
      "model1=MultinomialNB()\n",
      "model1.fit(bag_of_words, df.polarity)\n",
      "\n",
      "predict = model1.predict(bag_of_words)\n",
      "df['polarity_hat']=np.array(predict) # a two step regression\n",
      "# the true polarity is a function of the predicted polarity\n",
      "\n",
      "# modelcovar = ['polarity_hat','percent_capitalized','exclamation','at','link']\n",
      "modelcovar = ['polarity_hat','percent_capitalized','exclamation','at','link']\n",
      "model2 = LogisticRegression()\n",
      "model2.fit(df[modelcovar],df.polarity)\n",
      "predictions = model2.predict(df[modelcovar])\n",
      "\n",
      "print accuracy_score(predictions, df['polarity']) # true score could be worse since this is only run on training set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.891347099312\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "class ModelTester:\n",
      "    def __init__(self):\n",
      "        self.models = []\n",
      "        \n",
      "    def add_model(self, name, modeltype, pre_processing = \"x\", vectorizer_args = {}, model_args={}):\n",
      "        vectorizer = CountVectorizer(**vectorizer_args)\n",
      "        model = modeltype(**model_args)\n",
      "        post_processing = df.tweet.apply(lambda x: eval(pre_processing))\n",
      "        bag_of_words = vectorizer.fit_transform(post_processing)\n",
      "        model.fit(bag_of_words, df.polarity)\n",
      "        data = {\n",
      "            'name':name,\n",
      "            'vectorizer':vectorizer,\n",
      "            'pre_processing': pre_processing,\n",
      "            'model': model\n",
      "        }\n",
      "        self.models.append(data)\n",
      "        \n",
      "    def test_models(self, tweet):\n",
      "        for model in self.models:\n",
      "            process = lambda x: eval(model['pre_processing'])\n",
      "            processed_tweet = process(tweet)\n",
      "            bag_of_words = model['vectorizer'].transform([processed_tweet])\n",
      "            print \"%s: %s\" % (model['name'],model['model'].predict(bag_of_words))\n",
      "        \n",
      "        \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeltester= ModelTester()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeltester.add_model(\"Jarret\", MultinomialNB, vectorizer_args={'max_features':587})\n",
      "modeltester.add_model(\"In Class Demo\", LogisticRegression, vectorizer_args={'ngram_range':(1,2)})\n",
      "modeltester.add_model(\"Susan\", LogisticRegression, vectorizer_args={'stop_words':'english','ngram_range':(1,3)})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeltester.test_models(\"MODEL TESTER IS AWESOME!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jarret: [4]\n",
        "In Class Demo: [4]\n",
        "Jarret: [4]\n",
        "In Class Demo: [4]\n",
        "Susan: [4]\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeltester.test_models(\"UGHHHH IT DOESNT LIKE LASSO!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jarret: [0]\n",
        "In Class Demo: [0]\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}